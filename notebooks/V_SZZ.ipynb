{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4hNxGGosiAL",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title install srcml\n",
        "!wget https://github.com/srcML/srcMLReleases/raw/main/srcml_1.0.0-1_ubuntu20.04.deb\n",
        "!sudo apt install ./srcml_1.0.0-1_ubuntu20.04.deb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVD_1hi9Ixp-"
      },
      "source": [
        "### Install V-SZZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCYhwlDD8Ai_"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/baolingfeng/V-SZZ.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehrUKS088p7o"
      },
      "outputs": [],
      "source": [
        "%cd /content/V-SZZ/ICSE2022ReplicationPackage/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDhNJmBhDFBk"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBZNo3JVCs9e"
      },
      "source": [
        "### Files overwrite/fix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHgX24t-9Y6-"
      },
      "outputs": [],
      "source": [
        "%%writefile identify_duplicated_patch.py\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import subprocess\n",
        "import re\n",
        "import hashlib\n",
        "import git # get diff patch on Windows \n",
        "from unidiff import PatchSet\n",
        "from io import StringIO\n",
        "from log_generation import GitLog\n",
        "\n",
        "from setting import *\n",
        "\n",
        "from git_analysis.analyze_git_logs import retrieve_git_logs, retrieve_git_logs_dict, get_ancestors, get_parent_tags, get_son_tags\n",
        "from data_loader import JAVA_CVE_FIX_COMMITS, C_CVE_FIX_COMMITS, read_cve_commits, REPOS_DIR, JAVA_PROJECTS, C_PROJECTS, ANNOTATED_CVES\n",
        "\n",
        "repos_dir = REPOS_DIR\n",
        "log_dir = LOG_DIR\n",
        "\n",
        "def clear_patched_file(patched_file):\n",
        "    results = []\n",
        "    for line in patched_file.split('\\n'):\n",
        "        if line.startswith('index '):\n",
        "            continue\n",
        "        \n",
        "        # ignore the line with line information since some cherry picked patches have different line number\n",
        "        if line.startswith('@@'):\n",
        "            continue\n",
        "        \n",
        "        results.append(line)\n",
        "    \n",
        "    return '\\n'.join(results)\n",
        "\n",
        "def is_target_file(file_path):\n",
        "    splitted_path_tokens = file_path.lower().split('/')\n",
        "\n",
        "    file_name = splitted_path_tokens[-1]\n",
        "    idx = file_name.find('.')\n",
        "    if idx <= 0:\n",
        "        return False\n",
        "    \n",
        "    suffix = file_name[idx+1:]\n",
        "    if suffix not in ['java', 'c', 'cpp', 'h', 'hpp']:\n",
        "        return False\n",
        "    \n",
        "    if 'test' in splitted_path_tokens:\n",
        "        return False\n",
        "    \n",
        "    if file_name.startswith('test') or file_name.endswith('test'):\n",
        "        return False\n",
        "    \n",
        "    return True\n",
        "    \n",
        "def genereate_hashes_for_patch(repository, commit_id):\n",
        "    try:\n",
        "        uni_diff_text = repository.git.diff(commit_id+ '~1', commit_id,\n",
        "                                        ignore_blank_lines=True, \n",
        "                                        ignore_space_at_eol=True)\n",
        "        \n",
        "        patch_set = PatchSet(StringIO(uni_diff_text))\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return None\n",
        "    \n",
        "    hashes = []\n",
        "    for patched_file in patch_set:\n",
        "        file_path = patched_file.path\n",
        "        if not is_target_file(file_path):\n",
        "            continue\n",
        "\n",
        "        content = clear_patched_file(str(patched_file))\n",
        "        # print(content)\n",
        "        h = hashlib.sha1(content.encode('utf-8', 'ignore')).hexdigest()\n",
        "\n",
        "        hashes.append(h)\n",
        "    \n",
        "    return hashes\n",
        "\n",
        "def identify_duplicate_patch(project):\n",
        "    git_logs = retrieve_git_logs(os.path.join(log_dir, project+\"-meta.log\"), project)\n",
        "    \n",
        "    project_path = os.path.join(repos_dir, project)\n",
        "    repository = git.Repo(project_path)\n",
        "\n",
        "    commit_patch_map = {}\n",
        "    for gl in git_logs:\n",
        "        print(gl.commit_id)\n",
        "        hashes = genereate_hashes_for_patch(repository, gl.commit_id)\n",
        "        if hashes is not None:\n",
        "            commit_patch_map[gl.commit_id] = hashes\n",
        "    \n",
        "    patch_commit_map = {}\n",
        "    for commit_id in commit_patch_map:\n",
        "        for h in commit_patch_map[commit_id]:\n",
        "            if h in patch_commit_map:\n",
        "                patch_commit_map[h].append(commit_id)\n",
        "            else:\n",
        "                patch_commit_map[h] = [commit_id]\n",
        "    \n",
        "    return commit_patch_map, patch_commit_map\n",
        "\n",
        "def batch_duplicate_detection(projects):\n",
        "    # for project in C_PROJECTS:\n",
        "    for project in projects:\n",
        "        try:\n",
        "            commit_patch_map, patch_commit_map = identify_duplicate_patch(project)\n",
        "            with open(f'data_commit_patch_map/{project}-commit-patch.json', 'w') as fout1, \\\n",
        "                open(f'data_commit_patch_map/{project}-patch-commit.json', 'w') as fout2:\n",
        "                json.dump(commit_patch_map, fout1, indent=4)\n",
        "                json.dump(patch_commit_map, fout2, indent=4)\n",
        "        except Exception as e:\n",
        "            print(project, e)\n",
        "        else:\n",
        "            pass\n",
        "        \n",
        "        break\n",
        "   \n",
        "if __name__ == '__main__':\n",
        "    # Generate hashes for hunks in commits\n",
        "    batch_duplicate_detection()\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnosXxsP9fl-"
      },
      "outputs": [],
      "source": [
        "%%writefile setting.py\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# config your working folder and the correponding folder\n",
        "WORK_DIR = '/content/V-SZZ/ICSE2022ReplicationPackage/'\n",
        "\n",
        "REPOS_DIR = '/data1/baolingfeng/repos'\n",
        "\n",
        "DATA_FOLDER = os.path.join(WORK_DIR, 'data')\n",
        "\n",
        "SZZ_FOLDER = os.path.join(WORK_DIR, 'icse2021-szz-replication-package')\n",
        "\n",
        "DEFAULT_MAX_CHANGE_SIZE = sys.maxsize\n",
        "\n",
        "AST_MAP_PATH = os.path.join(WORK_DIR, 'ASTMapEval_jar')\n",
        "\n",
        "LOG_DIR = os.path.join(WORK_DIR, 'GitLogs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCvvocW59Azr"
      },
      "outputs": [],
      "source": [
        "%%writefile main.py\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import logging as log\n",
        "from numba import jit, cuda\n",
        "from setting import *\n",
        "\n",
        "sys.path.append(os.path.join(SZZ_FOLDER, 'tools/pyszz/'))\n",
        "\n",
        "from szz.ag_szz import AGSZZ\n",
        "from szz.b_szz import BaseSZZ\n",
        "from szz.l_szz import LSZZ\n",
        "from szz.ma_szz import MASZZ, DetectLineMoved\n",
        "from szz.r_szz import RSZZ\n",
        "from szz.ra_szz import RASZZ\n",
        "from szz.pd_szz import PyDrillerSZZ\n",
        "from szz.my_szz import MySZZ\n",
        "from tempfile import mkdtemp\n",
        "from data_loader import JAVA_CVE_FIX_COMMITS, C_CVE_FIX_COMMITS, JAVA_PROJECTS, C_PROJECTS, read_cve_commits\n",
        "from multiprocessing import Pool\n",
        "import multiprocessing\n",
        "from functools import partial\n",
        "from collections import ChainMap\n",
        "import numpy as np\n",
        "def find_vul(commits, project, repo_url, REPOS_DIR, use_temp_dir, AST_MAP_PATH, EXT_TO_PARSE):\n",
        "          my_szz = MySZZ(repo_full_name=project, repo_url=repo_url, repos_dir=REPOS_DIR, use_temp_dir=use_temp_dir, ast_map_path=mkdtemp(dir=AST_MAP_PATH))\n",
        "          max_progress = len(commits)\n",
        "          progress=0 \n",
        "          output = {}\n",
        "          pid = os.getpid()\n",
        "          for commit in commits:\n",
        "            try:\n",
        "                print('Fixing Commit:', commit)\n",
        "                imp_files = my_szz.get_impacted_files(fix_commit_hash=commit, file_ext_to_parse=EXT_TO_PARSE, only_deleted_lines=True)\n",
        "                bug_introducing_commits = my_szz.find_bic(fix_commit_hash=commit,\n",
        "                                        impacted_files=imp_files,\n",
        "                                        ignore_revs_file_path=None)\n",
        "                output[commit] = bug_introducing_commits\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "            progress+=1\n",
        "            print('\\033[1m\\033[92m[%s] progress %s%%\\033[0m' % (pid, 100*progress/max_progress))\n",
        "          return output\n",
        "          \n",
        "\n",
        "  \n",
        "\n",
        "def run_szz(project, commits, method, repo_url=None, max_change_size=DEFAULT_MAX_CHANGE_SIZE, languages=[]):\n",
        "    output_file = \"results/{method}-{project}.json\".format(method=method, project=project)\n",
        "\n",
        "    if os.path.exists(output_file):\n",
        "        return\n",
        "    use_temp_dir = False\n",
        "\n",
        "    EXT_TO_PARSE = list(set(['c', 'java', 'cpp', 'h', 'hpp', 'js', 'ts'] + languages))\n",
        "\n",
        "    output = {}\n",
        "\n",
        "    global max_progress\n",
        "    max_progress=len(commits)\n",
        "\n",
        "    if method == \"b\":\n",
        "        b_szz = BaseSZZ(repo_full_name=project, repo_url=repo_url, repos_dir=REPOS_DIR)\n",
        "        for commit in commits:\n",
        "            print('Fixing Commit:', commit)\n",
        "            imp_files = b_szz.get_impacted_files(fix_commit_hash=commit, file_ext_to_parse=EXT_TO_PARSE, only_deleted_lines=True)\n",
        "            bug_introducing_commits = b_szz.find_bic(fix_commit_hash=commit,\n",
        "                                      impacted_files=imp_files,\n",
        "                                      ignore_revs_file_path=None)\n",
        "            output[commit] = [commit.hexsha for commit in bug_introducing_commits]\n",
        "    elif method == \"ag\":\n",
        "        ag_szz = AGSZZ(repo_full_name=project, repo_url=repo_url, repos_dir=REPOS_DIR, use_temp_dir=use_temp_dir)\n",
        "        for commit in commits:\n",
        "            print('Fixing Commit:', commit)\n",
        "            imp_files = ag_szz.get_impacted_files(fix_commit_hash=commit, file_ext_to_parse=EXT_TO_PARSE, only_deleted_lines=True)\n",
        "            bug_introducing_commits = ag_szz.find_bic(fix_commit_hash=commit,\n",
        "                                      impacted_files=imp_files,\n",
        "                                      ignore_revs_file_path=None,\n",
        "                                      max_change_size=max_change_size)\n",
        "            output[commit] = [commit.hexsha for commit in bug_introducing_commits]\n",
        "    elif method == \"ma\":\n",
        "        ma_szz = MASZZ(repo_full_name=project, repo_url=repo_url, repos_dir=REPOS_DIR, use_temp_dir=use_temp_dir)\n",
        "        for commit in commits:\n",
        "            print('Fixing Commit:', commit)\n",
        "            imp_files = ma_szz.get_impacted_files(fix_commit_hash=commit, file_ext_to_parse=EXT_TO_PARSE, only_deleted_lines=True)\n",
        "            bug_introducing_commits = ma_szz.find_bic(fix_commit_hash=commit,\n",
        "                                      impacted_files=imp_files,\n",
        "                                      ignore_revs_file_path=None,\n",
        "                                      max_change_size=max_change_size)\n",
        "\n",
        "            output[commit] = [commit.hexsha for commit in bug_introducing_commits]\n",
        "    elif method == \"my\":\n",
        "        \n",
        "        MySZZ(repo_full_name=project, repo_url=repo_url, repos_dir=REPOS_DIR, use_temp_dir=use_temp_dir, ast_map_path=AST_MAP_PATH) #to already clone repo\n",
        "        cpu_count = multiprocessing.cpu_count()+1\n",
        "        print(cpu_count)\n",
        "        pp = Pool(cpu_count)\n",
        "        \n",
        "        output = dict(ChainMap(*pp.map(partial(find_vul, project=project, repo_url=repo_url, REPOS_DIR=REPOS_DIR, use_temp_dir=use_temp_dir, AST_MAP_PATH=AST_MAP_PATH, EXT_TO_PARSE=EXT_TO_PARSE), np.array_split(commits, multiprocessing.cpu_count()*2))))\n",
        "    elif method == \"ra\":\n",
        "        ra_szz = RASZZ(repo_full_name=project, repo_url=repo_url, repos_dir=REPOS_DIR, use_temp_dir=use_temp_dir)\n",
        "        for commit in commits:\n",
        "            print('Fixing Commit:', commit)\n",
        "            imp_files = ra_szz.get_impacted_files(fix_commit_hash=commit, file_ext_to_parse=EXT_TO_PARSE, only_deleted_lines=True)\n",
        "            bug_introducing_commits = ra_szz.find_bic(fix_commit_hash=commit,\n",
        "                                      impacted_files=imp_files,\n",
        "                                      ignore_revs_file_path=None,\n",
        "                                      max_change_size=max_change_size)\n",
        "            output[commit] = [commit.hexsha for commit in bug_introducing_commits]\n",
        "\n",
        "    with open(output_file, 'w') as fout:\n",
        "        json.dump(output, fout, indent=4)\n",
        "\n",
        "def load_annotated_commits(target_projects=None):\n",
        "    with open(os.path.join(DATA_FOLDER, 'inputs.json')) as fin:\n",
        "        annotation = json.load(fin)\n",
        "\n",
        "        project_commits = []\n",
        "        for project in annotation:\n",
        "            project_name, project_url, fixing_commits, languages = project\n",
        "            if target_projects is not None and project_name not in target_projects:\n",
        "                continue\n",
        "\n",
        "            project_commits.append(project)\n",
        "\n",
        "        return project_commits\n",
        "import sys\n",
        "if __name__ == \"__main__\":\n",
        "    use_temp_dir = False\n",
        "\n",
        "    # fixing_commits = JAVA_CVE_FIX_COMMITS\n",
        "    # fixing_commits = C_CVE_FIX_COMMITS\n",
        "\n",
        "    project_commits = load_annotated_commits()      \n",
        "    project_name, project_url, fixing_commits, languages = project_commits[int(sys.argv[2])]\n",
        "    print(\"Project:\", project_name, sys.argv[2])\n",
        "    try:\n",
        "      #assuming all are github repo\n",
        "      run_szz(project_name, fixing_commits, sys.argv[1], repo_url='https://github.com/%s' % project_url, languages=languages)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      print('can\\'t run szz on %s' % project_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL2cp_XzCY8E"
      },
      "outputs": [],
      "source": [
        "%%writefile icse2021-szz-replication-package/tools/pyszz/szz/my_szz.py\n",
        "import os\n",
        "import sys\n",
        "import logging as log\n",
        "import traceback\n",
        "from typing import List, Set\n",
        "import subprocess\n",
        "import json\n",
        "\n",
        "from git import Commit\n",
        "\n",
        "from szz.core.abstract_szz import AbstractSZZ, ImpactedFile\n",
        "\n",
        "from pydriller import ModificationType, GitRepository as PyDrillerGitRepo\n",
        "import Levenshtein\n",
        "\n",
        "\n",
        "def remove_whitespace(line_str):\n",
        "    return ''.join(line_str.strip().split())\n",
        "\n",
        "def compute_line_ratio(line_str1, line_str2):\n",
        "    l1 = remove_whitespace(line_str1)\n",
        "    l2 = remove_whitespace(line_str2)\n",
        "    return Levenshtein.ratio(l1, l2)\n",
        "\n",
        "MAXSIZE = sys.maxsize\n",
        "\n",
        "class MySZZ(AbstractSZZ):\n",
        "    \"\"\"\n",
        "    My SZZ implementation.\n",
        "\n",
        "    Supported **kwargs:\n",
        "\n",
        "    * ignore_revs_file_path\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, repo_full_name: str, repo_url: str, repos_dir: str = None, use_temp_dir: bool = True, ast_map_path = None):\n",
        "        super().__init__(repo_full_name, repo_url, repos_dir)\n",
        "        self.ast_map_path = ast_map_path\n",
        "        self.repo_full_name = repo_full_name\n",
        "\n",
        "    def find_bic(self, fix_commit_hash: str, impacted_files: List['ImpactedFile'], **kwargs) -> Set[Commit]:\n",
        "        \"\"\"\n",
        "        Find bug introducing commits candidates.\n",
        "\n",
        "        :param str fix_commit_hash: hash of fix commit to scan for buggy commits\n",
        "        :param List[ImpactedFile] impacted_files: list of impacted files in fix commit\n",
        "        :key ignore_revs_file_path (str): specify ignore revs file for git blame to ignore specific commits.\n",
        "        :returns Set[Commit] a set of bug introducing commits candidates, represented by Commit object\n",
        "        \"\"\"\n",
        "\n",
        "        log.info(f\"find_bic() kwargs: {kwargs}\")\n",
        "\n",
        "        ignore_revs_file_path = kwargs.get('ignore_revs_file_path', None)\n",
        "        # self._set_working_tree_to_commit(fix_commit_hash)\n",
        "\n",
        "        bug_introd_commits = []\n",
        "        for imp_file in impacted_files:\n",
        "            # print('impacted file', imp_file.file_path)\n",
        "            try:\n",
        "                blame_data = self._blame(\n",
        "                    # rev='HEAD^',\n",
        "                    rev='{commit_id}^'.format(commit_id=fix_commit_hash),\n",
        "                    file_path=imp_file.file_path,\n",
        "                    modified_lines=imp_file.modified_lines,\n",
        "                    ignore_revs_file_path=ignore_revs_file_path,\n",
        "                    ignore_whitespaces=False,\n",
        "                    skip_comments=True\n",
        "                )\n",
        "\n",
        "                for entry in blame_data:\n",
        "                    print(entry.commit, entry.line_num, entry.line_str)\n",
        "                    previous_commits = []\n",
        "                    \n",
        "                    blame_result = entry\n",
        "                    while True:\n",
        "                        if imp_file.file_path.endswith(\".java\"):\n",
        "                            mapped_line_num, change_type = self.map_modified_line_java(blame_result, imp_file.file_path)\n",
        "                            previous_commits.append((blame_result.commit.hexsha, blame_result.line_num, blame_result.line_str, change_type))\n",
        "                        else:\n",
        "                            mapped_line_num = self.map_modified_line(blame_result, imp_file.file_path)\n",
        "                            previous_commits.append((blame_result.commit.hexsha, blame_result.line_num, blame_result.line_str))\n",
        "                        \n",
        "                        if mapped_line_num == -1:\n",
        "                            break\n",
        "                        \n",
        "                        \n",
        "                        blame_data2 = self._blame(\n",
        "                                        rev='{commit_id}^'.format(commit_id=blame_result.commit.hexsha),\n",
        "                                        file_path=imp_file.file_path,\n",
        "                                        modified_lines=[mapped_line_num],\n",
        "                                        ignore_revs_file_path=ignore_revs_file_path,\n",
        "                                        ignore_whitespaces=False,\n",
        "                                        skip_comments=True\n",
        "                                    )\n",
        "                        blame_result = list(blame_data2)[0]\n",
        "                        # print(blame_result.commit.hexsha, blame_result.line_num)\n",
        "                        # print(mapped_line_num, blame_result.commit.hexsha, blame_result.line_num, blame_result.line_str)\n",
        "                        # previous_commits.append((blame_result.commit, blame_result.line_num, blame_result.line_str))\n",
        "\n",
        "                    # bug_introd_commits[entry.line_num] = {'line_str': entry.line_str, 'file_path': entry.file_path, 'previous_commits': previous_commits}\n",
        "                    bug_introd_commits.append({'line_num':entry.line_num, 'line_str': entry.line_str, 'file_path': entry.file_path, 'previous_commits': previous_commits})\n",
        "                    # bug_introd_commits.append(previous_commits)\n",
        "            except:\n",
        "                print(traceback.format_exc())\n",
        "\n",
        "        return bug_introd_commits\n",
        "\n",
        "    def map_modified_line_java(self, blame_entry, blame_file_path):\n",
        "        mapping_cmd = \"java -jar ASTMapEval.jar -p {project} -c {commit_id} -o {output} -f {file_path}\"\n",
        "        ast_map_temp = os.path.join(self.ast_map_path, 'temp')\n",
        "\n",
        "        commit_id = blame_entry.commit.hexsha\n",
        "        file_path = blame_file_path.replace('\\\\', '/')\n",
        "        \n",
        "        line_num = blame_entry.line_num\n",
        "\n",
        "        mapping_db = None\n",
        "        mapping_db_file = os.path.join(ast_map_temp, \"{project}.json\".format(project=self.repo_full_name))\n",
        "        if os.path.exists(mapping_db_file):\n",
        "            mapping_db = json.load(open(mapping_db_file))\n",
        "            if commit_id not in mapping_db:\n",
        "                subprocess.check_output(mapping_cmd.format(project=self.repo_full_name, commit_id=commit_id, output=os.path.join(ast_map_temp, \"tmp.json\"), file_path=file_path), cwd=self.ast_map_path, shell=True).decode('utf-8', errors='ignore')\n",
        "\n",
        "                mapping_results = json.load(open(os.path.join(ast_map_temp, \"tmp.json\")))\n",
        "                mapping_db[commit_id] = {}\n",
        "                mapping_db[commit_id][file_path] = mapping_results\n",
        "            elif file_path not in mapping_db[commit_id]:\n",
        "                subprocess.check_output(mapping_cmd.format(project=self.repo_full_name, commit_id=commit_id, output=os.path.join(ast_map_temp, \"tmp.json\"), file_path=file_path), cwd=self.ast_map_path, shell=True).decode('utf-8', errors='ignore')\n",
        "\n",
        "                mapping_results = json.load(open(os.path.join(ast_map_temp, \"tmp.json\")))\n",
        "                mapping_db[commit_id][file_path] = mapping_results\n",
        "            else:\n",
        "                mapping_results = mapping_db[commit_id][file_path]\n",
        "        else:\n",
        "            subprocess.check_output(mapping_cmd.format(project=self.repo_full_name, commit_id=commit_id, output=os.path.join(ast_map_temp, \"tmp.json\"), file_path=file_path), cwd=self.ast_map_path, shell=True).decode('utf-8', errors='ignore')\n",
        "\n",
        "            mapping_results = json.load(open(os.path.join(ast_map_temp, \"tmp.json\")))\n",
        "            mapping_db = {}\n",
        "            mapping_db[commit_id] = {}\n",
        "            mapping_db[commit_id][file_path] = mapping_results\n",
        "\n",
        "        with open(mapping_db_file, 'w') as fout:\n",
        "            json.dump(mapping_db, fout, indent=4)\n",
        "\n",
        "        target_file = None\n",
        "        target_stmt = None\n",
        "        for result in mapping_results:\n",
        "            if file_path == result['src']:\n",
        "                target_file = result['dst']\n",
        "                      \n",
        "                for stmt in result['stmt']:\n",
        "                    if 'dstStmtStartLine' in stmt and stmt['dstStmtStartLine'] == int(line_num):\n",
        "                        target_stmt = stmt\n",
        "                        break\n",
        "                \n",
        "                if target_stmt is not None:\n",
        "                    break\n",
        "        \n",
        "        if target_stmt is None:\n",
        "            # \"New File\"\n",
        "            return -1, \"New File\"\n",
        "        \n",
        "        # results.append((buggy_commit, buggy_file, buggy_line, target_stmt['stmtChangeType']))\n",
        "        if target_stmt['stmtChangeType'] == \"Insert\":\n",
        "            return -1, target_stmt['stmtChangeType']\n",
        "        \n",
        "        return target_stmt['srcStmtStartLine'], target_stmt['stmtChangeType']\n",
        "       \n",
        "\n",
        "    def map_modified_line(self, blame_entry, blame_file_path):\n",
        "        #TODO: rename type \n",
        "        blame_commit = PyDrillerGitRepo(self.repository_path).get_commit(blame_entry.commit.hexsha)\n",
        "        # print('get blame commit', blame_commit, blame_entry.commit.hexsha)\n",
        "\n",
        "        for mod in blame_commit.modifications:\n",
        "            file_path = mod.new_path\n",
        "            if mod.change_type == ModificationType.DELETE or mod.change_type == ModificationType.RENAME:\n",
        "                file_path = mod.old_path\n",
        "\n",
        "            if file_path != blame_file_path:\n",
        "                continue\n",
        "\n",
        "            if not mod.old_path:\n",
        "                # \"newly added\"\n",
        "                return -1\n",
        "\n",
        "            lines_added = [added for added in mod.diff_parsed['added']]\n",
        "            lines_deleted = [deleted for deleted in mod.diff_parsed['deleted']]\n",
        "\n",
        "            if len(lines_deleted) == 0:\n",
        "                return -1\n",
        "            \n",
        "            print('line added/deleted', len(lines_added), len(lines_deleted))\n",
        "\n",
        "            if blame_entry.line_str:\n",
        "                sorted_lines_deleted = [(line[0], line[1], \n",
        "                                            compute_line_ratio(blame_entry.line_str, line[1]), \n",
        "                                            abs(blame_entry.line_num - line[0])) \n",
        "                                        for line in lines_deleted]\n",
        "                sorted_lines_deleted = sorted(sorted_lines_deleted, key=lambda x : (x[2], MAXSIZE-x[3]), reverse=True)\n",
        "                # print(sorted_lines_deleted)\n",
        "                \n",
        "                # print(sorted_lines_deleted)\n",
        "                if sorted_lines_deleted[0][2] > 0.75:\n",
        "                    return sorted_lines_deleted[0][0]\n",
        "                                             \n",
        "        return -1        \n",
        "                \n",
        "                    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inBMbGz5h5zw"
      },
      "source": [
        "### Convert Verified vulnerability inducing dataset (small dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMgM2fV-c22Z"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "repomap = {'FFmpeg':'FFmpeg/FFmpeg', 'ImageMagick':'ImageMagick/ImageMagick', 'linux-kernel':'torvalds/linux', \n",
        "           'OpenSSL': 'openssl/openssl', 'php-src': 'php/php-src'}\n",
        "with open('/content/V-SZZ/ICSE2022ReplicationPackage/data/verified_cve_with_versions_C.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "  outputs = {}\n",
        "  for cve in data:\n",
        "    name = cve['project']\n",
        "    if not name in outputs:\n",
        "      outputs[name] = [name, repomap[name], [], None]\n",
        "    outputs[name][2] += [fix['fixing_commit'] for fix in cve['fixing_details']]\n",
        "  \n",
        "  with open('./data/inputs.json', 'w') as out:\n",
        "    json.dump(list(outputs.values()), out, indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n7ovTuyDaPL"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us3jYwMQDdQX"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/V-SZZ/ICSE2022ReplicationPackage/ASTMapEval_jar/temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG4R-TX9leVS"
      },
      "source": [
        "### Run SZZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqk-TnKcldh_"
      },
      "outputs": [],
      "source": [
        "%cd /content/V-SZZ/ICSE2022ReplicationPackage/icse2021-szz-replication-package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyY-Etfuli9Z"
      },
      "outputs": [],
      "source": [
        "!python ../main.py my 0"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SVD_1hi9Ixp-",
        "YBZNo3JVCs9e",
        "inBMbGz5h5zw",
        "0n7ovTuyDaPL"
      ],
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}